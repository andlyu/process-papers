{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_paper_url(title):\n",
    "    # Replace spaces with '+' for the query\n",
    "    query = '+'.join(title.split())\n",
    "    url = f\"https://scholar.google.com/scholar?q={query}\"\n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Parse the response using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Find the link to the paper (this will depend on the structure of the page)\n",
    "        # This is an example and might not work as Google Scholar's layout changes\n",
    "        div_gs_or_ggsm = soup.find('div', class_='gs_or_ggsm')\n",
    "\n",
    "        # Check if the div is found and it contains an <a> tag\n",
    "        if div_gs_or_ggsm and div_gs_or_ggsm.find('a'):\n",
    "            # Extract the href attribute\n",
    "            link = div_gs_or_ggsm.find('a')['href']\n",
    "            return link\n",
    "        else:\n",
    "            print(\"The link could not be found.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve results: Status code {response.status_code}\")\n",
    "\n",
    "# Example usage\n",
    "paper_title = \"Attention is All You Need\"\n",
    "paper_url = get_paper_url(paper_title)\n",
    "print(paper_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [\n",
    "    \"Self-Supervised Object Detection from Egocentric Videos\",\n",
    "    \"STEPs: Self-Supervised Key Step Extraction and Localization from Unlabeled Procedural Videos\",\n",
    "    \"Masked Spatio-Temporal Structure Prediction for Self-supervised Learning on Point Cloud Videos\",\n",
    "    \"Learning Trajectory-Word Alignments for Video-Language Tasks\",\n",
    "    \"Spatio-Temporal Crop Aggregation for Video Representation Learning\",\n",
    "    \"Progressive Spatio-Temporal Prototype Matching for Text-Video Retrieval\",\n",
    "    \"Long-range Multimodal Pretraining for Movie Understanding\",\n",
    "    \"MGMAE: Motion Guided Masking for Video Masked Autoencoding\",\n",
    "    \"Learning to Ground Instructional Articles in Videos through Narrations\",\n",
    "    \"Accurate and Fast Compressed Video Captioning\",\n",
    "    \"Few-Shot Video Classification via Representation Fusion and Promotion Learning\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Supervised Object Detection from Egocentric Videos\n",
      "https://openaccess.thecvf.com/content/ICCV2023/papers/Akiva_Self-Supervised_Object_Detection_from_Egocentric_Videos_ICCV_2023_paper.pdf\n",
      "\n",
      "STEPs: Self-Supervised Key Step Extraction and Localization from Unlabeled Procedural Videos\n",
      "https://openaccess.thecvf.com/content/ICCV2023/papers/Shah_STEPs_Self-Supervised_Key_Step_Extraction_and_Localization_from_Unlabeled_Procedural_ICCV_2023_paper.pdf\n",
      "\n",
      "Masked Spatio-Temporal Structure Prediction for Self-supervised Learning on Point Cloud Videos\n",
      "http://openaccess.thecvf.com/content/ICCV2023/papers/Shen_Masked_Spatio-Temporal_Structure_Prediction_for_Self-supervised_Learning_on_Point_Cloud_ICCV_2023_paper.pdf\n",
      "\n",
      "Learning Trajectory-Word Alignments for Video-Language Tasks\n",
      "https://arxiv.org/pdf/2301.01953\n",
      "\n",
      "Spatio-Temporal Crop Aggregation for Video Representation Learning\n",
      "http://openaccess.thecvf.com/content/ICCV2023/papers/Sameni_Spatio-Temporal_Crop_Aggregation_for_Video_Representation_Learning_ICCV_2023_paper.pdf\n",
      "\n",
      "Progressive Spatio-Temporal Prototype Matching for Text-Video Retrieval\n",
      "https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Progressive_Spatio-Temporal_Prototype_Matching_for_Text-Video_Retrieval_ICCV_2023_paper.pdf\n",
      "\n",
      "Long-range Multimodal Pretraining for Movie Understanding\n",
      "https://openaccess.thecvf.com/content/ICCV2023/papers/Argaw_Long-range_Multimodal_Pretraining_for_Movie_Understanding_ICCV_2023_paper.pdf\n",
      "\n",
      "MGMAE: Motion Guided Masking for Video Masked Autoencoding\n",
      "https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_MGMAE_Motion_Guided_Masking_for_Video_Masked_Autoencoding_ICCV_2023_paper.pdf\n",
      "\n",
      "Learning to Ground Instructional Articles in Videos through Narrations\n",
      "https://arxiv.org/pdf/2306.03802\n",
      "\n",
      "Accurate and Fast Compressed Video Captioning\n",
      "https://openaccess.thecvf.com/content/ICCV2023/papers/Shen_Accurate_and_Fast_Compressed_Video_Captioning_ICCV_2023_paper.pdf\n",
      "\n",
      "Few-Shot Video Classification via Representation Fusion and Promotion Learning\n",
      "https://openaccess.thecvf.com/content/ICCV2023/papers/Xia_Few-Shot_Video_Classification_via_Representation_Fusion_and_Promotion_Learning_ICCV_2023_paper.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for paper in titles:\n",
    "    print(paper)\n",
    "    paper_url = get_paper_url(paper)\n",
    "    print(paper_url)\n",
    "    time.sleep(5)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract text from pdf\n",
    "pdf_url = \"data/Accurate and Fast Compressed Video Captioning.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/Accurate and Fast Compressed Video Captioning.txt'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "def convert_pdf_to_txt(pdf_path):\n",
    "    extracted_text = extract_text(pdf_path)\n",
    "    text_path  = pdf_url[:-4] + '.txt'\n",
    "    with open(text_path, 'w') as f:\n",
    "        f.write(extracted_text)\n",
    "\n",
    "    return text_path\n",
    "\n",
    "convert_pdf_to_txt(pdf_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurate and Fast Compressed Video Captioning\n",
      "\n",
      "Yaojie Shen1,2∗, Xin Gu1,2∗, Kai Xu3, Heng Fan4, Longyin Wen3, Libo Zhang1,2†\n",
      "1 Institute of Software, Chinese Academy of Sciences, Beijing, China\n",
      "2 University of Chinese Academy of Sciences, Beijing, China\n",
      "3 ByteDance Inc., San Jose, USA\n",
      "4 Department of Computer Science and Engineering, University of North Texas, Denton TX, USA\n",
      "\n",
      "Abstract\n",
      "\n",
      "Existing video captioning approaches typically require\n",
      "to first sample video frames from a decoded video and\n",
      "then conduct a subsequent process (e.g., feature extraction\n",
      "and/or captioning model learning). In this pipeline, manual\n",
      "frame sampling may ignore key information in videos and\n",
      "thus degrade performance. Additionally, redundant infor-\n",
      "mation in the sampled frames may result in low efficiency in\n",
      "the inference of video captioning. Addressing this, we study\n",
      "video captioning from a different perspective in compressed\n",
      "domain, which brings multi-fold advantages over the exist-\n",
      "ing pipeline: 1) Compared to raw images from the decoded\n",
      "video, the compressed video, consisting of I-frames, mo-\n",
      "tion vectors and residuals, is highly distinguishable, which\n",
      "allows us to leverage the entire video for learning with-\n",
      "out manual sampling through a specialized model design;\n",
      "2) The captioning model is more efficient in inference as\n",
      "smaller and less redundant information is processed. We\n",
      "propose a simple yet effective end-to-end transformer in the\n",
      "compressed domain for video captioning that enables learn-\n",
      "ing from the compressed video for captioning. We show that\n",
      "even with a simple design, our method can achieve state-of-\n",
      "the-art performance on different benchmarks while running\n",
      "almost 2× faster than existing approaches. Code is avail-\n",
      "able at https://github.com/acherstyx/CoCap.\n",
      "\n",
      "1. Introduction\n",
      "\n",
      "Video captioning is a representative example of applying\n",
      "deep learning to the fields of computer vision and natural\n",
      "language processing with a long list of applications, such\n",
      "as blind navigation, video event commentary, and human-\n",
      "computer interaction. To generate captions for a video, the\n",
      "model needs to not only identify objects and actions in the\n",
      "video, but also be able to express them accurately in natural\n",
      "\n",
      "*The two authors make equal contributions and are co-first authors.\n",
      "†Corresponding author: Libo Zhang (libo@iscas.ac.cn).\n",
      "\n",
      "Figure 1. Comparing our method with prior methods for video cap-\n",
      "tioning. Prior works are all based on decoding video frames. The\n",
      "difference between them is that some methods use offline extracted\n",
      "multiple features as input and generate captions, while others di-\n",
      "rectly take dense video frames as input. By avoiding heavy re-\n",
      "dundant information and offline multiple feature extraction, our\n",
      "method speedup the caption generation process while maintaining\n",
      "high quality results.\n",
      "\n",
      "language. Despite significant progress, accurate and fast\n",
      "video captioning remains a challenge.\n",
      "\n",
      "Video captioning requires both 2D appearance informa-\n",
      "tion, which reflects the objects in the video, and 3D action\n",
      "information, which reflects the actions. The interaction be-\n",
      "tween these two types of information is crucial for accu-\n",
      "rately captioning the actions of objects in the video. Most\n",
      "of the existing methods [36, 38, 22] are shown in Fig. 1\n",
      "(the upper branch), mainly including the three-steps: (1)\n",
      "Decoding the video and densely sampling frames. (2) Ex-\n",
      "tracting the 2D/3D features of the video frames offline. (3)\n",
      "Training the model based on these 2D/3D features. In these\n",
      "methods, densely sampled video frames result in signifi-\n",
      "cant redundancy, which in turn increases the computation\n",
      "and inference time of the model. This is because the model\n",
      "needs to extract features from each video frame and use all\n",
      "\n",
      "CompressedVideoRGB Frames2D/3D FeaturesCaption Wordsa piece of garlicis being choppedRGB Framesa piece of garlicis being choppeda piece of garlicis being chopped(c) Our one-step approach(b) Two-step approachVideoDecodingVideoDecodingFeatureExtractorCaptionModelEnd-to-End Caption ModelEnd-to-End Caption Model in Compressed DomainCaption WordsCaption Words(a) Three-step approachThisICCVpaperistheOpenAccessversion,providedbytheComputerVisionFoundation.Exceptforthiswatermark,itisidenticaltotheacceptedversion;thefinalpublishedversionoftheproceedingsisavailableonIEEEXplore.15558\fr\n",
      "E\n",
      "D\n",
      "C\n",
      "\n",
      "I\n",
      "\n",
      "58\n",
      "\n",
      "56\n",
      "\n",
      "54\n",
      "\n",
      "52\n",
      "\n",
      "50\n",
      "\n",
      "48\n",
      "\n",
      "0.1\n",
      "\n",
      "Ours(I+MV+Res, L-14)\n",
      "\n",
      "Ours(I+MV+Res)\n",
      "\n",
      "Ours(I+MV)\n",
      "\n",
      "Ours(I)\n",
      "\n",
      "SwinBERT [18]\n",
      "\n",
      "HMN [36]\n",
      "\n",
      "10\n",
      "\n",
      "SGN [27]\n",
      "\n",
      "1\n",
      "Second\n",
      "\n",
      "Figure 2. Comparison of model inference speed and CIDEr score\n",
      "on MSRVTT dataset. I, MV and Res refer to I-frame, motion vec-\n",
      "tor and residual respectively. The test is run on 1 Card V100 ma-\n",
      "chine with batch size set to 1.\n",
      "\n",
      "of these features as input. Furthermore, extracting 2D ap-\n",
      "pearance features, 3D action features, and region features\n",
      "for each video frame requires additional time. To address\n",
      "the speed issue and improve inference speed, some recent\n",
      "works [18, 29] have adopted an end-to-end approach that\n",
      "avoids extracting multiple visual features offline. As shown\n",
      "in Fig. 1 (The middle branch), the flow of their method is as\n",
      "follows: (1) Decoding the video and densely sample frames.\n",
      "(2) Take video frames directly as input and then end-to-end\n",
      "training model. These approaches involve a trainable vi-\n",
      "sual feature extractor, rather than relying on multiple offline\n",
      "2D/3D feature extractors. For example, SwinBERT [18]\n",
      "uses VidSwin [19] as the trainable feature extractor, while\n",
      "MV-GPT [29] uses ViViT [1]. While these two-steps meth-\n",
      "ods address the time consumption associated with offline\n",
      "feature extraction, they do not alleviate the computational\n",
      "burden and time required to handle the redundancy of infor-\n",
      "mation.\n",
      "\n",
      "To address the above problems, we propose an end-to-\n",
      "end video captioning method based on compressed video.\n",
      "Our work significantly simplifies the video caption pipeline\n",
      "by eliminating time-consuming video decoding and feature\n",
      "extraction steps. As in Fig. 1 (the lower branch), unlike\n",
      "previous methods, we take compressed video information\n",
      "as input and directly output a natural language description\n",
      "of the video. Compressed video is mainly composed of I-\n",
      "frame, motion vector and residual, and there is no redun-\n",
      "dant information between them, and they are all refined in-\n",
      "formation. Therefore, the model needs less computation to\n",
      "process compressed domain information, and model infer-\n",
      "ence is faster. At the same time, the end-to-end network\n",
      "structure in our proposed method can also avoid the time\n",
      "consumption caused by extracting multiple features. Be-\n",
      "sides, Our model is better at understanding the content of\n",
      "videos by utilizing the refined information in compressed\n",
      "domain, including the 2D feature from I-frame and the 3D\n",
      "action feature extracted from motion vector and residual.\n",
      "\n",
      "As shown in Fig. 2, compared with other two-steps and\n",
      "three-steps methods, such as SwinBERT [18], HMN [36]\n",
      "and SGN [27], our method is not only faster, but also has\n",
      "competitive performance. Our model comprises two parts,\n",
      "as depicted in Fig. 4. One part consists of three encoders\n",
      "that extract features and an action encoder that fuses them,\n",
      "while the other part comprises a multimodal decoder that\n",
      "generates video captions. Specifically, we first extract the\n",
      "context feature, motion vector feature and residual feature\n",
      "of the compressed video through I-frame Encoder, Motion\n",
      "Encoder, and Residual Encoder, respectively. The context\n",
      "feature contains information about objects in the video, but\n",
      "action information is missing.\n",
      "In order to extract the ac-\n",
      "tion feature of the video, we fuse the motion vector feature,\n",
      "residual feature, and context feature through the action en-\n",
      "coder. Then use the context feature and action feature as\n",
      "visual input of the multimodal decoder to generate video\n",
      "captions.\n",
      "\n",
      "The contributions of this paper are summarized below:\n",
      "\n",
      "1. We propose a simple and effective transformer that can\n",
      "take compressed video as input and directly generate a\n",
      "video description.\n",
      "\n",
      "2. Our experimental results demonstrate that our method is\n",
      "nearly 2× further than the fastest existing state-of-the-art\n",
      "method in inference time, while maintaining competitive\n",
      "results on three challenging video captioning datasets,\n",
      "e.g., MSVD, MSRVTT and VATEX.\n",
      "\n",
      "2. Related Work\n",
      "\n",
      "Compressed vision task. The main idea of introducing\n",
      "compressed video into current computer vision tasks is to\n",
      "utilizing the motion vector and residual on the compressed\n",
      "domain to avoid fully decode all frames from the video and\n",
      "save the storage space at the same time. Early work mainly\n",
      "base on MPEG-4 video codec [33, 16, 12, 4]. CoViAR [33]\n",
      "proposed a back-tracking technique to trace motion vectors\n",
      "back to I-frame, which works on MPEG-4. MM-ViT [4]\n",
      "proposed a multi-modal transformer to process the I-frame,\n",
      "motion vector, residual and audio in the compressed video.\n",
      "Since the MPEG-4 codec is outdated, other works, e.g.,\n",
      "MVCGC [13] and ATTP [14] , is designed to work on\n",
      "other coedcs like H.264 and H.265 to ensure generaliz-\n",
      "ability. Comparing with MPEG-4, H.264 and H.265 al-\n",
      "low a more flexible yet complicated compression, which\n",
      "makes it more challenging to learn from compressed do-\n",
      "main. MVCGC [13] proposed a self-supervised method\n",
      "to learn video representations by utilizing the mutual in-\n",
      "formation between RGB video frames and motion vectors.\n",
      "ATTP [14] designed a lightweight deep neural network to\n",
      "process the compressed video and achieve real time action\n",
      "recognition on embedded AI devices. Similarly, our work\n",
      "\n",
      "15559\fis conducted on H.264 video codec, which is currently one\n",
      "of the most popular video codecs.\n",
      "Video captioning. Video captioning aims to convert the\n",
      "content of videos into natural language descriptions, which\n",
      "requires the model to understand the objects in the video\n",
      "and the behavior of the objects. Some works focus on\n",
      "the design of the model structure. These methods usu-\n",
      "ally extract features offline, and then models use these fea-\n",
      "tures to generate captions by designing different network\n",
      "architectures. HMN [36] proposed a hierarchical modu-\n",
      "lar network that serves as a strong video encoder, which\n",
      "bridges videos and languages. ORG-TRL [38] proposes\n",
      "an object relational graph based encoder, which captures\n",
      "more detailed interaction features to enrich visual represen-\n",
      "tation. SGN [27] designed a semantic grouping network\n",
      "to group video frames with discriminating word phrases\n",
      "of partially decoded caption. Some works explore addi-\n",
      "tional information to help the model generate more accu-\n",
      "rate video captions. TextKG [9] propose a two-stream net-\n",
      "work capable of knowledge-assisted video description us-\n",
      "ing knowledge graphs. Univl [20] learns powerful vision-\n",
      "and-language representations by pre-training the models on\n",
      "large-scale datasets, e.g., HowTo100M [21] and WebVid-\n",
      "2M [2]. Some other works focus more on end-to-end video\n",
      "captioning generation. SwinBERT [18] proposed an end-\n",
      "to-end transformer-based model, which takes video frame\n",
      "patches directly as inputs and then uses VidSwin to extract\n",
      "visual features. MV-GPT [29] designed an encoder-decoder\n",
      "model end-to-end to generate the video caption from video\n",
      "frames and transcribed speech directly. We propose an end-\n",
      "to-end video captioning model based on the compressed do-\n",
      "main without decoding video frames and extracting features\n",
      "offline, which not only accelerates the generation of cap-\n",
      "tions, but also performs favorably against the state-of-the-\n",
      "art methods.\n",
      "\n",
      "3. Methods\n",
      "\n",
      "As mentioned above, our method aims to take the dense\n",
      "information (including I-frame, motion vector and residual)\n",
      "in compressed domain as input to accelerate inference and\n",
      "improve performance for video caption. To this end, we de-\n",
      "sign an end-to-end transformer-based network as shown in\n",
      "Fig. 4. In this section, we first detail the information in the\n",
      "compressed video in Sec. 3.1, then introduce the model net-\n",
      "work in Sec. 3.2 and 3.3, and finally introduce the training\n",
      "strategy of the model in Sec. 3.4.\n",
      "\n",
      "3.1. The Structure of Compressed Video\n",
      "\n",
      "Modern video codecs utilizing the temporal redundancy\n",
      "of successive video frames to compress raw video. As\n",
      "shown in Fig. 3, most modern codecs (e.g., H.264, and\n",
      "H.265) divide video frames into three different types ac-\n",
      "cording to their dependencies with other frames: I-frame\n",
      "\n",
      "Figure 3. The GOP structure in compressed video. In each GOP,\n",
      "the first frame must be an I-frame, followed by several B/P-frames.\n",
      "\n",
      "(intra coded frame), P-frame (predictive coded frame) and\n",
      "B-frame (bipredictive coded frame).\n",
      "I-frame is fully en-\n",
      "coded independently using intra-prediction without relying\n",
      "on other frames. Other frames like B-frame and P-frame\n",
      "are encoded by referring to the other frames using inter-\n",
      "prediction, which is stored in the form of motion vector.\n",
      "Motion vector describes the movement of a group of pixels\n",
      "from source (reference frames) to destination (current B-\n",
      "frame or P-frame), which contains highly compressed mo-\n",
      "tion information of successive video frames. The difference\n",
      "between P-frame and B-frame is that B-frame could refer to\n",
      "the frames before or after it, while P-frame only refer to the\n",
      "frames before it. Since predicting a frame using neighbor-\n",
      "ing frames could be inaccurate, an additional residual error\n",
      "between the current frame and the prediction is calculated.\n",
      "We denote II , IP and IB as decoded I-frame, P-frame, and\n",
      "B-frame, and Imv and ∆res as the motion vector and resid-\n",
      "ual of P-/B-frame respectively. In compressed domain, the\n",
      "P-frame and B-frame could be reconstructed by\n",
      "\n",
      "IB/P = Pred(Imv, Iref ) + ∆res\n",
      "\n",
      "(1)\n",
      "\n",
      "where Iref is the referenced frame, and Pred is the predic-\n",
      "tion method to reconstruct current frame based on motion\n",
      "vector and referenced frame. Since the reconstruction pro-\n",
      "cess is time consuming, our model takes highly compressed\n",
      "information from compressed domain directly as input to\n",
      "achieve end-to-end video captioning.\n",
      "\n",
      "Moreover, successive frames are divided into several\n",
      "groups, which is called Groups of Pictures (GOP). GOP\n",
      "is an independent encoding or decoding unit, which means\n",
      "that the frames in a GOP do not refer to any frames on other\n",
      "GOP. Each GOP starts with an I frame, followed by sev-\n",
      "eral P-frames or B-frames. For each GOP, we take one I-\n",
      "frame and M B-/P-frames as inputs. The B-/P-frames are\n",
      "uniformly sampled from each GOP, and we only use their\n",
      "motion vector and residual as replacements. Therefore, the\n",
      "visual inputs of our model would be\n",
      "\n",
      "X = [I (1)\n",
      "\n",
      "I\n",
      ". . . , [I (N )\n",
      "\n",
      "I\n",
      "\n",
      ", I (1,1)\n",
      "\n",
      "mv , ∆(1,1)\n",
      ", I (1,N )\n",
      "mv\n",
      "\n",
      "res , . . . , I (M,1)\n",
      ", ∆(1,N )\n",
      "res\n",
      "\n",
      "mv\n",
      "\n",
      ", . . . , I (M,N )\n",
      "\n",
      "],\n",
      "\n",
      ", ∆(M,1)\n",
      "res\n",
      ", ∆(M,N )\n",
      "res\n",
      "\n",
      "mv\n",
      "\n",
      "]\n",
      "\n",
      "where N is the number of GOP sampled from each video\n",
      "and M is the total number of P-/B-frames sampled from\n",
      "each GOP. We set N according to the average GOP number,\n",
      "\n",
      "I-frameP-frameP-frameB-frame...GOP #1P-frameI-frameGOP #2...P-frame: Motion...: Group of Pixels: Video FramesGOP #N15560\fand M is equal to the maximum number of P-/B-frames in\n",
      "each GOP, which is a hyper-parameter during encoding.\n",
      "\n",
      "3.2. Model Architecture for Compressed Domain\n",
      "\n",
      "Based on the GOP structure mentioned above, we pro-\n",
      "posed a transformer based structure to utilizing the dense\n",
      "information from the compressed domain. Fig. 4 (left)\n",
      "shows the main framework of our proposed compressed\n",
      "video transformer. The model takes all information of the\n",
      "compressed video as inputs, including I-frame, motion vec-\n",
      "tor and residual, while maintaining a fast inference speed.\n",
      "Specifically, we use three different Vision Transformers [8]\n",
      "(ViT) as encoder to extract the visual features for I-frame,\n",
      "motion vector and residual. We adopt a pretrained Vision\n",
      "Transformer as the encoder to extract the context feature\n",
      "from the I-frame:\n",
      "\n",
      "ctx = EncoderI(I (n)\n",
      "F (n)\n",
      "\n",
      "I\n",
      "\n",
      ").\n",
      "\n",
      "For each B-frame or P-frame, we get a motion vector\n",
      "and a residual from the compressed domain. We use two\n",
      "lightweight Vision Transformers as encoders to extract fea-\n",
      "tures from motion vectors and residuals. The motion and\n",
      "residual features is added together to generate the B-/P-\n",
      "frame features F (m,n)\n",
      "\n",
      ":\n",
      "\n",
      "BP\n",
      "\n",
      "F (m,n)\n",
      "\n",
      "BP = Encodermv(I (m,n)\n",
      "\n",
      "mv\n",
      "\n",
      ") + Encoderres(∆(m,n)\n",
      "\n",
      "res\n",
      "\n",
      ").\n",
      "\n",
      "In this way, for each GOP we obtain M B-/P-frame features\n",
      "\n",
      "BP = [F (1,n)\n",
      "F (n)\n",
      "\n",
      "BP , . . . , F (M,n)\n",
      "\n",
      "BP\n",
      "\n",
      "].\n",
      "\n",
      "As motion vector and residual lack fine-grained context\n",
      "information, we use features from motion vector and resid-\n",
      "ual as queries to retrieve the rich context information in\n",
      "RGB frames instead of simply fusing them. We employ ac-\n",
      "tion encoder to integrate the object information of I-frame\n",
      "into the action information of motion vector and residual,\n",
      "which takes B-/P-frame features in current GOP F (n)\n",
      "BP and\n",
      "the context feature F (n)\n",
      "ctx as input to generate the action fea-\n",
      "ture F (n)\n",
      "act of current GOP. The action encoder is constructed\n",
      "by Na sets of alternately stacked self-attention and cross-\n",
      "attention blocks.\n",
      "\n",
      "Specifically, the workflow of the action encoder is as fol-\n",
      "lows. Firstly, according to the reconstruction process de-\n",
      "scribed in Eq. 1, we utilize the self-attention module fuse\n",
      "the temporal representation of successive frames to obtain\n",
      "F (n)\n",
      "att :\n",
      "\n",
      "X = F (n)\n",
      "\n",
      "BP + Embp + Embt,\n",
      "\n",
      "Q = Wq ∗ X, K = Wk ∗ X, V = Wv ∗ X,\n",
      "\n",
      "F (n)\n",
      "\n",
      "att = SelfAttention(Q, K, V ),\n",
      "\n",
      "where Embp is the positional embeddings, Embt is the type\n",
      "embeddings, and Wq, Wk, Wv are learnable matrices. The\n",
      "type embeddings are added to distinguish B-frames and P-\n",
      "frames. And then we use the cross-attention to integrate the\n",
      "ctx from I-frame into the F (n)\n",
      "F (n)\n",
      "att from the motion vector and\n",
      "residual. Finally, the action feature F (n)\n",
      "act\n",
      "\n",
      "Q′ = W ′\n",
      "\n",
      "q ∗ F (n)\n",
      "\n",
      "att , K ′ = W ′\n",
      "\n",
      "k ∗ F (n)\n",
      "\n",
      "ctx , V ′ = W ′\n",
      "\n",
      "v ∗ F (n)\n",
      "ctx ,\n",
      "\n",
      "F (n)′\n",
      "att = CrossAttention(Q′, K ′, V ′),\n",
      "act = Mean(F (n)′\n",
      "F (n)\n",
      "att ),\n",
      "\n",
      "where W ′\n",
      "function that calculates the average feature.\n",
      "\n",
      "v are learnable matrices and Mean() is a\n",
      "\n",
      "k, W ′\n",
      "\n",
      "q, W ′\n",
      "\n",
      "3.3. Multimodal Decoder for Video Captioning\n",
      "ctx and action features F (n)\n",
      "\n",
      "act for\n",
      "each GOP are contacted to form the visual representation:\n",
      "\n",
      "The context features F (n)\n",
      "\n",
      "V = [F (1)\n",
      "\n",
      "ctx, F (1)\n",
      "\n",
      "act , . . . , F (N )\n",
      "\n",
      "ctx , F (N )\n",
      "act ].\n",
      "\n",
      "Then we design a multimodal decoder to predict the\n",
      "video captions based on the visual representation V. The\n",
      "multimodal decoder is composed of Nm masked self-\n",
      "attention modules stacked as shown in Fig. 4 (right) and\n",
      "the workflow is as follows:\n",
      "\n",
      "T<t = Embedding(Y<t),\n",
      "\n",
      "X = Concat(V, T<t),\n",
      "\n",
      "Q′′ = W ′′\n",
      "\n",
      "X ′ = X + Emb′\n",
      "\n",
      "p + Emb′\n",
      "t,\n",
      "k ∗ X ′, V ′′ = W ′′\n",
      "q ∗ X ′, K ′′ = W ′′\n",
      "ht = MaskedSelfAttention(Q′′, K ′′, V ′′),\n",
      "\n",
      "v ∗ X ′,\n",
      "\n",
      "p(yt|V, T<t) = softmax(Linear(ht)),\n",
      "\n",
      "where Y<t is the words generated in previous t − 1 steps,\n",
      "Embedding() is a function that converts one-hot word vec-\n",
      "tors into word embeddings, Emb′\n",
      "p is the positional embed-\n",
      "dings, Emb′\n",
      "t is used to distinguish different modality of in-\n",
      "puts, W ′′\n",
      "k , W ′′\n",
      "v are learnable matrices and yt is the pre-\n",
      "diction of current step. In the multimodal decoder, position\n",
      "embedding and type embedding is added to distinguish the\n",
      "order and type of features respectively.\n",
      "\n",
      "q , W ′′\n",
      "\n",
      "3.4. Optimization\n",
      "\n",
      "We train our model using the cross-entropy loss func-\n",
      "tion. Given the ground-truth indices of previous (t-1) words\n",
      "and the visual representation V, we can get the predictions\n",
      "of the current t-th word y∗\n",
      "t . After that, the training loss is\n",
      "computed as\n",
      "\n",
      "L = − (cid:80)l\n",
      "\n",
      "t=1 log p(y∗\n",
      "\n",
      "t |y∗\n",
      "\n",
      ":t−1, V),\n",
      "\n",
      "15561\fFigure 4. The architecture of our proposed Compressed Video Captioner. Left: The Compressed Video Transformer which extract video\n",
      "representation for each GOP. A large visual backbone is used to extract visual representations from I-frame, and two small Vision Trans-\n",
      "former is used to extract residual and motion representations from compressed domain. After that, an action encoder is used to fuse the\n",
      "features. Right: The Multimodal Decoder. We use a multimodal decoder with causal mask to learn caption.\n",
      "\n",
      "where y∗\n",
      "1:T is the ground truth sequence and l is the to-\n",
      "tal length of predicted captions. Notably, we add the label\n",
      "smoothing to mitigate overconfidence in implementation.\n",
      "\n",
      "4. Experiments\n",
      "\n",
      "4.1. Datasets\n",
      "\n",
      "MSRVTT [34] is a generic video captioning dataset that\n",
      "comprises 10, 000 video clips, with each clip annotated with\n",
      "20 captions. On average, each video clip lasts about 15 sec-\n",
      "onds. The standard split involves the use of 6, 513 clips for\n",
      "training, 497 clips for validation, and 2, 990 clips for test-\n",
      "ing.\n",
      "MSVD [3] contains 1, 970 videos, with each video clip hav-\n",
      "ing 40 captions. The average duration of each video clip is\n",
      "around 10 seconds. We adopt the standard split, which in-\n",
      "volves using 1, 200 videos for training, 100 videos for vali-\n",
      "dation, and 670 videos for testing.\n",
      "VATEX [32] is a large-scale dataset which contains about\n",
      "41, 250 video clips. The duration of each video clip is be-\n",
      "tween 10 seconds, and 10 English captions are manually\n",
      "annotated per clip. We use the official training set for train-\n",
      "ing and evaluate the results using the public test set.\n",
      "\n",
      "4.2. Evaluation Metrics\n",
      "\n",
      "To evaluate the effectiveness of our approach, we use the\n",
      "standard metrics for video captioning: BLEU@4 (B4) [23],\n",
      "\n",
      "METEOR (M) [7], ROUGE (R) [17], and CIDEr (C) [31].\n",
      "Each metric provides a unique perspective on the quality of\n",
      "the generated captions. BLEU@4 evaluates sentence flu-\n",
      "ency, METEOR assesses semantic accuracy, ROUGE mea-\n",
      "sures word order, and CIDEr evaluates the degree to which\n",
      "the caption conveys key information. By considering these\n",
      "different metrics, we can comprehensively evaluate the per-\n",
      "formance of our model.\n",
      "\n",
      "4.3. Implementation Details\n",
      "\n",
      "Our model is implemented using PyTorch, and to read\n",
      "motion vectors and residuals from the compressed video,\n",
      "we utilize the x264 library in FFmpeg. Before training and\n",
      "testing, the videos are resized to 240 on its smallest edge\n",
      "and compressed using the H.264 codec with KeyInt set to\n",
      "60. For each video, we fixedly sampled 8 GOPs, each of\n",
      "which contains 1 I-frame, 59 motion vectors, and 59 resid-\n",
      "uals. The size of the I-frame and residual is 3 ∗ 224 ∗ 224,\n",
      "and the size of the motion vector is 4 ∗ 56 ∗ 56. We use\n",
      "Adam with initial learning rate of 1e-4, β1=0.9, β2=0.999\n",
      "and the warmup strategy is adopted in the training. The\n",
      "maximum length of the caption sentence is set to 22, which\n",
      "contains two special tokens, e.g., [CLS] token and [EOS]\n",
      "token. The feature dimension in each block is set to 768,\n",
      "and the number of heads in multi-head architecture is set to\n",
      "12 for all layers. The batch size is set to 64 and the train-\n",
      "ing epochs to 20. The I-frame encoder has 12 layers and is\n",
      "\n",
      "I-frameEncoder[CLS].........ActionEncoder...I-FrameB-/P-FrameGOPMultimodal DecoderVisualCaption[BOS]amancountrysidethedrivesthroughavehicle...amancountrysidethedrivesthroughavehicle[EOS]: Add: Concat: MeanPrediction: Input: Copy...ResidualEncoderMotionEncoder...[CLS].........ResidualMotion Vector[CLS]15562\fMethod\n",
      "\n",
      "Decoding E2E\n",
      "\n",
      "SAAT [39]\n",
      "STG-KD [22]\n",
      "PMI-CAP [5]\n",
      "ORG-TRL [38]\n",
      "OpenBook [37]\n",
      "SGN [27]\n",
      "MGRMP [6]\n",
      "HMN [36]\n",
      "UniVL [20]\n",
      "SwinBERT [18]\n",
      "MV-GPT [29]\n",
      "Ours\n",
      "Ours(ViT/L14)\n",
      "\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "\n",
      "-\n",
      "-\n",
      "\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "\n",
      "2D Appearance 3D Action Object Detection B4 M\n",
      "IncepResnetV2\n",
      "ResNet101\n",
      "IncepResnetV2\n",
      "IncepResnetV2\n",
      "IncepResnetV2\n",
      "ResNet101\n",
      "IncepResnetV2\n",
      "IncepResnetV2\n",
      "\n",
      "-\n",
      "FasterRCNN\n",
      "-\n",
      "FasterRCNN\n",
      "-\n",
      "-\n",
      "-\n",
      "FasterRCNN\n",
      "\n",
      "-\n",
      "\n",
      "MSVD\n",
      "R\n",
      "\n",
      "MSRVTT\n",
      "R\n",
      "\n",
      "-\n",
      "\n",
      "-\n",
      "\n",
      "-\n",
      "\n",
      "C\n",
      "81.0\n",
      "93.0\n",
      "95.1\n",
      "95.2\n",
      "-\n",
      "94.3\n",
      "98.5\n",
      "104\n",
      "-\n",
      "\n",
      "46.5 33.5 69.4\n",
      "52.2 36.9 73.9\n",
      "54.6 36.4\n",
      "54.3 36.4 73.9\n",
      "-\n",
      "52.8 35.5 72.9\n",
      "55.8 36.9 74.5\n",
      "59.2 37.7 75.1\n",
      "-\n",
      "\n",
      "C\n",
      "B4 M\n",
      "39.9 27.7 61.2\n",
      "51\n",
      "40.5 28.3 60.9 47.1\n",
      "49.4\n",
      "42.1 28.7\n",
      "43.6 28.8 62.1 50.9\n",
      "42.8 29.3 61.7 52.9\n",
      "40.8 28.3 60.8 49.5\n",
      "41.7 28.9 62.1 51.4\n",
      "43.5\n",
      "62.7 51.5\n",
      "42.2 28.8 61.2 49.9\n",
      "58.2 41.3 77.5 120.6 41.9 29.9 62.1 53.8\n",
      "60\n",
      "55.9 39.9 76.8 113.0 43.1 29.8 62.7 56.2\n",
      "60.1 41.4 78.2 121.5 44.4 30.3 63.4 57.2\n",
      "\n",
      "48.9 38.7\n",
      "\n",
      "64\n",
      "\n",
      "29\n",
      "\n",
      "-\n",
      "\n",
      "-\n",
      "\n",
      "-\n",
      "\n",
      "-\n",
      "\n",
      "-\n",
      "\n",
      "-\n",
      "\n",
      "Features\n",
      "\n",
      "C3D\n",
      "I3D\n",
      "C3D\n",
      "C3D\n",
      "C3D\n",
      "C3D\n",
      "C3D\n",
      "C3D\n",
      "S3D\n",
      "VidSwin\n",
      "ViViT\n",
      "CLIP\n",
      "CLIP\n",
      "\n",
      "Table 1. Comparison with state-of-the-art methods on the test split of MSVD and MSRVTT. Decoding means decoding video frames, and\n",
      "E2E means end-to-end training without offline feature extraction. For a fair comparison, we gray out models that pre-train on large-scale\n",
      "datasets.\n",
      "\n",
      "NITS-VC [30]\n",
      "VATEX [32]\n",
      "ORG-TRL [38]\n",
      "Support-set [24]\n",
      "SwinBERT [18]\n",
      "VideoCoCa [35]\n",
      "Ours\n",
      "Ours(ViT/L14)\n",
      "\n",
      "B4\n",
      "20.0\n",
      "28.4\n",
      "32.1\n",
      "32.8\n",
      "38.7\n",
      "39.7\n",
      "31.4\n",
      "35.8\n",
      "\n",
      "M\n",
      "18.0\n",
      "21.7\n",
      "22.2\n",
      "24.4\n",
      "26.2\n",
      "-\n",
      "23.2\n",
      "25.3\n",
      "\n",
      "R\n",
      "42.0\n",
      "47\n",
      "48.9\n",
      "49.1\n",
      "53.2\n",
      "54.5\n",
      "49.4\n",
      "52.0\n",
      "\n",
      "C\n",
      "24.0\n",
      "45.1\n",
      "49.7\n",
      "51.2\n",
      "73\n",
      "77.8\n",
      "52.7\n",
      "64.8\n",
      "\n",
      "Table 2. Comparison with state-of-the-art methods on the test split\n",
      "of VATEX. For a fair comparison, we gray out models that pre-\n",
      "train on large-scale datasets.\n",
      "\n",
      "initialized with pre-trained weights from the CLIP [25] vi-\n",
      "sual encoder, while the other encoders and the multimodal\n",
      "decoder are randomly initialized. The layers for the motion\n",
      "encoder, residual encoder and action encoder are 2, 2 and 1,\n",
      "respectively. Lastly, we set the hyperparameters M , N , Na,\n",
      "and Nm to 60, 8, 2 and 2.\n",
      "\n",
      "4.4. Performance Comparison with SOTA Methods\n",
      "\n",
      "In order to verify the effectiveness of the method, we\n",
      "evaluated the proposed model against state-of-the-art meth-\n",
      "ods on three public benchmark datasets.\n",
      "MSVD dataset. The evaluation results on the MSVD\n",
      "dataset are reported in Table 1 (left). We conducted experi-\n",
      "ments using two sizes of the I-frame encoder, namely B/16\n",
      "and L/14, with the results reported in the article based on\n",
      "B/16, unless otherwise stated. Our method using the L/14\n",
      "I-frame encoder achieves the best performance on all met-\n",
      "rics, with only SwinBERT [18] performing better than our\n",
      "\n",
      "method using B/16. Our approach stands out by being able\n",
      "to directly utilize compressed domain information and ex-\n",
      "tract visual features in real-time. The result shows that our\n",
      "model can efficiently extract information from the refined\n",
      "compressed domain information.\n",
      "\n",
      "MSRVTT dataset.\n",
      "In the MSRVTT benchmark, our\n",
      "method outperforms other approaches in all metrics, as\n",
      "shown in Table 1 (right). Specifically, both the based on\n",
      "B/16 model and based on L/14 model achieve higher\n",
      "scores compared to other methods.\n",
      "In particular, our\n",
      "method achieves a CIDEr score of 56.2 / 57.2, which rep-\n",
      "resents a significant improvement of +2.4 / +3.4. This re-\n",
      "sult demonstrates that our approach can generate captions\n",
      "with higher semantic accuracy than other methods based on\n",
      "video decoding [31]. CIDEr is particularly effective at cap-\n",
      "turing human consensus, which makes our achievement in\n",
      "this metric even more impressive.\n",
      "\n",
      "VATEX dataset. Our method is evaluated on a large-scale\n",
      "dataset, as shown in Table 2. We achieve the second-best\n",
      "results on all metrics, falling behind SwinBERT [18]. Our\n",
      "approach involves extracting visual features using three Vi-\n",
      "sion Transformer encoders, while the I-frame encoder is ini-\n",
      "tialized with the pre-trained CLIP [25] model on LAION-\n",
      "In contrast, SwinBERT uses the VidSwin\n",
      "400M [28].\n",
      "backbone [19], which is pre-trained on the Kinetic-600\n",
      "dataset [15].\n",
      "It is worth noting that LAION-400M is a\n",
      "large image-text dataset, while Kinetics-600 is a video-text\n",
      "dataset, and VATEX dataset is a subset of Kinetics-600\n",
      "videos. SwinBERT outperforms our method on VATEX due\n",
      "to its backbone pre-trained on Kinetics-600.\n",
      "\n",
      "15563\fMethod\n",
      "\n",
      "SGN\n",
      "HMN\n",
      "SwinBERT\n",
      "Ours\n",
      "Ours\n",
      "Ours\n",
      "\n",
      "Data Type\n",
      "\n",
      "RGB Video Frames\n",
      "RGB Video Frames\n",
      "RGB Video Frames\n",
      "I-frame\n",
      "I-frame+MV\n",
      "I-frame+MV+Res\n",
      "\n",
      "Inference Time ↓\n",
      "\n",
      "Feature Extraction\n",
      "303 ms\n",
      "2,710 ms\n",
      "\n",
      "Model Time\n",
      "275 ms\n",
      "108 ms\n",
      "\n",
      "339 ms\n",
      "146 ms\n",
      "153 ms\n",
      "178 ms\n",
      "\n",
      "Total\n",
      "578 ms\n",
      "2,818 ms\n",
      "339 ms\n",
      "146 ms\n",
      "153 ms\n",
      "178 ms\n",
      "\n",
      "CIDEr ↑\n",
      "\n",
      "49.5\n",
      "51.5\n",
      "53.8\n",
      "54.1\n",
      "55.3\n",
      "56.2\n",
      "\n",
      "Table 3. A detailed comparison of speed with other methods on the test split of the MSRVTT dataset. During the test, the model is running\n",
      "on a NVIDIA Tesla V100 GPU and the batch size is set to 1. The time cost is computed on the overall MSRVTT test split.\n",
      "\n",
      "4.5. Speed Comparison with the SOTA Methods\n",
      "\n",
      "To evaluate the speed of our method, we compared it to\n",
      "three representative methods, namely SGN [27], HMN [36],\n",
      "and SwinBERT [18], as reported in Table 3. SGN is a three-\n",
      "step method that first decodes video frames and densely\n",
      "sample, then extracts the 2D appearance and 3D action fea-\n",
      "tures based on ResNet101 [11] and C3D [10] (consuming\n",
      "303 ms) offline, and finally uses the visual features as the\n",
      "input of the model (consuming 275 ms). Therefore, the total\n",
      "time for SGN to generate a video caption is 578 ms. HMN\n",
      "achieves the best results among the three-steps models, but\n",
      "it is relatively slow as it requires offline region feature ex-\n",
      "traction based on Faster RCNN [26] (consuming 2, 520 ms),\n",
      "leading to its total time of 2, 818 ms. SwinBERT, on the\n",
      "other hand, is an end-to-end method that does not extract\n",
      "multiple features offline, using only 339 ms.\n",
      "\n",
      "Compared to these methods, our proposed method does\n",
      "not require a dense sampling of video frames or the ex-\n",
      "traction of multiple features offline. As shown in Table 3,\n",
      "our baseline method only considers the I-frame of the entire\n",
      "video, achieving a CIDEr score of 54.1 and a total time of\n",
      "146 ms. By integrating the motion vector, we improved the\n",
      "CIDEr to 55.3, demonstrating that the action information in\n",
      "the motion vector helps the model generate captions. Fur-\n",
      "thermore, by incorporating residual information, the CIDEr\n",
      "score is further improved by 0.9 to reach 56.2. Although\n",
      "considering three inputs increases our total inference time,\n",
      "our speed is still nearly 2 times faster than SwinBERT, 3\n",
      "times faster than SGN, and 15 times faster than HMN.\n",
      "\n",
      "4.6. Ablation Study\n",
      "\n",
      "Impact of input information. To evaluate the effectiveness\n",
      "of different input information in our method, we conducted\n",
      "several experiments on the MSRVTT dataset, as shown in\n",
      "Table 4. To investigate the role of I-frame, motion vec-\n",
      "tor, and residual, we first experimented with using only\n",
      "one of them. As shown in Table 4, using only I-frame,\n",
      "motion vector, or residual achieved CIDEr scores of 54.1,\n",
      "19.4, and 13.0, respectively. This indicates that the model\n",
      "can directly use I-frame instead of motion vector and resid-\n",
      "\n",
      "Input\n",
      "Imv ∆res\n",
      "\n",
      "Module\n",
      "En A\n",
      "-\n",
      "-\n",
      "-\n",
      "✓\n",
      "✓\n",
      "-\n",
      "✓\n",
      "\n",
      "B4 M\n",
      "\n",
      "R\n",
      "\n",
      "C\n",
      "\n",
      "29.7\n",
      "41.6\n",
      "21.6\n",
      "27.3\n",
      "20.5\n",
      "23.9\n",
      "29.9\n",
      "43.4\n",
      "42.2\n",
      "30.0\n",
      "42.1 30.1\n",
      "43.1 29.8\n",
      "\n",
      "62.3 54.1\n",
      "52.6 19.4\n",
      "51.0 13.0\n",
      "62.6 55.3\n",
      "62.5 54.9\n",
      "62.4 54.3\n",
      "62.7 56.2\n",
      "\n",
      "-\n",
      "-\n",
      "✓\n",
      "-\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "\n",
      "II\n",
      "✓\n",
      "-\n",
      "✓\n",
      "-\n",
      "-\n",
      "-\n",
      "✓ ✓\n",
      "✓\n",
      "-\n",
      "✓ ✓\n",
      "✓ ✓\n",
      "\n",
      "Table 4. Ablation study of different input on the test subset of\n",
      "MSRVTT. The II , Imv and ∆res mean decoded I-frame, motion\n",
      "vector and residual respectively. And the En A means the action\n",
      "encoder.\n",
      "\n",
      "KeyInt (M ) GOP Nums (N ) Inference Time B4 M R\n",
      "\n",
      "C\n",
      "\n",
      "250\n",
      "60\n",
      "60\n",
      "60\n",
      "60\n",
      "\n",
      "2\n",
      "2\n",
      "4\n",
      "8\n",
      "10\n",
      "\n",
      "153 ms\n",
      "131 ms\n",
      "139 ms\n",
      "178 ms\n",
      "187 ms\n",
      "\n",
      "39.6 28.7 60.8 49.5\n",
      "41.6 29.3 61.7 52.4\n",
      "42.8 29.9 62.6 55.3\n",
      "43.1 29.8 62.7 56.2\n",
      "42.7 29.8 62.6 55.5\n",
      "\n",
      "Table 5. Ablation study of GOP numbers on MSRVTT test subset.\n",
      "\n",
      "ual. By jointly using I-frame and motion vector and fusing\n",
      "their information through the action encoder, we achieved\n",
      "a CIDEr score of 55.3. Similarly, using I-frame and resid-\n",
      "ual achieved a score of 54.9. This demonstrates that motion\n",
      "vector and residual can help the model generate more accu-\n",
      "rate captions. The performance of the model is further im-\n",
      "proved by inputting all three types of information, achieving\n",
      "a CIDEr score of 56.2, an improvement of 1.7. Removing\n",
      "the action encoder from the proposed method resulted in a\n",
      "slight drop in CIDEr scores, from 56.2 to 54.3. This demon-\n",
      "strates that the action encoder can help the model integrate\n",
      "the object information of I-frame into the action information\n",
      "of motion vector and residual.\n",
      "Impact of GOP numbers. GOP is a fundamental unit in\n",
      "compressed video that affect the compression rate. A larger\n",
      "GOP size results in fewer GOP numbers and commonly\n",
      "In video codec (e.g. FFmpeg),\n",
      "higher compression rates.\n",
      "the GOP size is determined by the KeyInt parameter. To\n",
      "\n",
      "15564\fFigure 5. Qualitative results on the MSRVTT, MSVD and VATEX dataset. We show the input of our model, which is in compressed\n",
      "domain. The red, green and blue borders indicate I-frame, motion vector and residual, respectively.\n",
      "\n",
      "En I En M En R En C De M CIDEr\n",
      "56.2\n",
      "12\n",
      "57.2\n",
      "24\n",
      "55.2\n",
      "12\n",
      "54.9\n",
      "12\n",
      "55.4\n",
      "12\n",
      "\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "4\n",
      "\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "4\n",
      "\n",
      "2\n",
      "2\n",
      "2\n",
      "4\n",
      "4\n",
      "\n",
      "Table 6. Ablation study about module layers on the MSRVTT test\n",
      "subset. En I, En M, En R, En A and De M refer to the I-frame en-\n",
      "coder, motion encoder, residual encoder, action encoder and mul-\n",
      "timodal decoder of the model respectively.\n",
      "\n",
      "investigate the impact of GOP size on our video caption\n",
      "model, we experimented with different GOP numbers and\n",
      "KeyInts, as shown in Table 5. Comparing KeyInt values of\n",
      "250 and 60, we observed that a smaller GOP size led to bet-\n",
      "ter model performance (49.5 CIDEr vs 52.4 CIDEr). By\n",
      "sampling different GOP numbers under the same KeyInt,\n",
      "the best performance is achieved by setting GOP size to 8\n",
      "and KeyInt to 60. While the performance is improved with\n",
      "more GOPs, yet speed is decreased due to increased com-\n",
      "putation as more information is included.\n",
      "\n",
      "Impact of model layers. To investigate the impact of dif-\n",
      "ferent model layers on our proposed method, we conducted\n",
      "an ablation study on the MSRVTT test subset, as shown\n",
      "in Table 6. Giving that I-frame contains more complex in-\n",
      "formation, we design a deep encoder with more layers for\n",
      "I-frame, while using a shallow encoder for motion vector\n",
      "and residual. Our results show that the performance of the\n",
      "model improves with an increase in the number of layers in\n",
      "the I-frame encoder (56.2 CIDEr to 57.2 CIDEr). However,\n",
      "adding more layers to other modules did not result in further\n",
      "\n",
      "improvements in model performance.\n",
      "\n",
      "4.7. Qualitative Results\n",
      "\n",
      "As shown in Fig. 5, we present the qualitative results\n",
      "of our proposed method on three datasets (e.g., MSVD,\n",
      "MSRVTT, and VATEX). Specifically, we visualize the in-\n",
      "put I-frame, motion vector, and residual and compare the\n",
      "predicted description to the ground truth. Our method con-\n",
      "sistently produces semantically consistent descriptions that\n",
      "closely align with the ground truth across all three datasets.\n",
      "Furthermore, the results demonstrate a superior ability to\n",
      "capture motion behavior in the videos.\n",
      "\n",
      "5. Conclusion\n",
      "\n",
      "In this paper, we introduce an end-to-end transformer-\n",
      "based model for video captioning that takes compressed\n",
      "video as input to eliminate redundant information. Our pro-\n",
      "posed method is evaluated on three challenging datasets and\n",
      "demonstrates that our proposed method is not only fast, but\n",
      "also competitive in performance with SOTA. In the future,\n",
      "we plan to further improve our method in two ways: (1) Add\n",
      "additional modalities such as audio, text, and knowledge\n",
      "graphs to enhance the quality of the generated captions. (2)\n",
      "Pre-train the model on a large-scale dataset to further boost\n",
      "the overall performance in compressed domain.\n",
      "\n",
      "Acknowledgement\n",
      "\n",
      "Libo Zhang was supported by Youth Innovation Promo-\n",
      "tion Association, CAS (2020111). Heng Fan and his em-\n",
      "ployer received no financial support for research, author-\n",
      "ship, and/or publication of this article. This work was done\n",
      "during internship at ByteDance Inc.\n",
      "\n",
      "MSRVTT Pred: a woman is talking about her kitchen GT: a girl is showing her kitchen......MSVD Pred: a man is chopping up some vegetables GT: a piece of garlic is being choppedVATEX Pred: a man is using a sander to clean a wooden floor GT: a guy showing how to use a floor sander on a wood floor................................................15565\fReferences\n",
      "\n",
      "[1] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen\n",
      "Sun, Mario Lucic, and Cordelia Schmid. Vivit: A video vi-\n",
      "sion transformer. In IEEE/CVF International Conference on\n",
      "Computer Vision, pages 6816–6826, 2021. 2\n",
      "\n",
      "[2] Max Bain, Arsha Nagrani, G¨ul Varol, and Andrew Zisser-\n",
      "man. Frozen in time: A joint video and image encoder for\n",
      "end-to-end retrieval. In IEEE/CVF International Conference\n",
      "on Computer Vision, pages 1708–1718, 2021. 3\n",
      "\n",
      "[3] David L. Chen and William B. Dolan. Collecting highly par-\n",
      "In Annual Meeting of\n",
      "\n",
      "allel data for paraphrase evaluation.\n",
      "the Association for Computational Linguistics, 2011. 5\n",
      "[4] Jiawei Chen and Chiu Man Ho. Mm-vit: Multi-modal video\n",
      "transformer for compressed video action recognition. In Pro-\n",
      "ceedings of the IEEE/CVF Winter Conference on Applica-\n",
      "tions of Computer Vision, pages 1910–1921, 2022. 2\n",
      "\n",
      "[5] Shaoxiang Chen, Wenhao Jiang, Wei Liu, and Yu-Gang\n",
      "Jiang. Learning modality interaction for temporal sentence\n",
      "In European\n",
      "localization and event captioning in videos.\n",
      "Conference on Computer Vision, pages 333–351, 2020. 6\n",
      "[6] Shaoxiang Chen and Yu-Gang Jiang. Motion guided region\n",
      "message passing for video captioning. In IEEE/CVF Inter-\n",
      "national Conference on Computer Vision, pages 1523–1532,\n",
      "2021. 6\n",
      "\n",
      "[7] Michael J. Denkowski and Alon Lavie. Meteor universal:\n",
      "Language specific translation evaluation for any target lan-\n",
      "guage. In Proceedings of the Ninth Workshop on Statistical\n",
      "Machine Translation, pages 376–380, 2014. 5\n",
      "\n",
      "[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\n",
      "Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\n",
      "Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\n",
      "vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\n",
      "worth 16x16 words: Transformers for image recognition at\n",
      "scale. In International Conference on Learning Representa-\n",
      "tions, 2021. 4\n",
      "\n",
      "[9] Xin Gu, Guang Chen, Yufei Wang, Libo Zhang, Tiejian Luo,\n",
      "and Longyin Wen. Text with knowledge graph augmented\n",
      "transformer for video captioning. In IEEE/CVF Conference\n",
      "on Computer Vision and Pattern Recognition, pages 18941–\n",
      "18951, 2023. 3\n",
      "\n",
      "[10] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can\n",
      "spatiotemporal 3d cnns retrace the history of 2d cnns and\n",
      "In IEEE/CVF Conference on Computer Vision\n",
      "imagenet?\n",
      "and Pattern Recognition, pages 6546–6555, 2018. 7\n",
      "\n",
      "[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n",
      "Deep residual learning for image recognition. In IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition,\n",
      "pages 770–778, 2016. 7\n",
      "\n",
      "[12] Lianghua Huang, Yu Liu, Bin Wang, Pan Pan, Yinghui Xu,\n",
      "and Rong Jin. Self-supervised video representation learning\n",
      "by context and motion decoupling. In IEEE/CVF Conference\n",
      "on Computer Vision and Pattern Recognition, pages 13886–\n",
      "13895, 2021. 2\n",
      "\n",
      "[13] Yuqi Huo, Mingyu Ding, Haoyu Lu, Nanyi Fei, Zhiwu Lu,\n",
      "Ji-Rong Wen, and Ping Luo. Compressed video contrastive\n",
      "learning. Advances in Neural Information Processing Sys-\n",
      "tems, 34:14176–14187, 2021. 2\n",
      "\n",
      "[14] Yuqi Huo, Xiaoli Xu, Yao Lu, Yulei Niu, Mingyu Ding,\n",
      "Zhiwu Lu, Tao Xiang, and Ji-rong Wen. Lightweight ac-\n",
      "tion recognition in compressed videos. In Computer Vision–\n",
      "ECCV 2020 Workshops: Glasgow, UK, August 23–28, 2020,\n",
      "Proceedings, Part II 16, pages 337–352. Springer, 2020. 2\n",
      "\n",
      "[15] Ang Li, Meghana Thotakuri, David A Ross, Jo˜ao Car-\n",
      "reira, Alexander Vostrikov, and Andrew Zisserman. The\n",
      "ava-kinetics localized human actions video dataset. arXiv\n",
      "preprint arXiv:2005.00214, 2020. 6\n",
      "\n",
      "[16] Jiapeng Li, Ping Wei, Yongchi Zhang, and Nanning Zheng.\n",
      "A slow-i-fast-p architecture for compressed video action\n",
      "recognition. In ACM International Conference on Multime-\n",
      "dia, pages 2039–2047, 2020. 2\n",
      "\n",
      "[17] Chin-Yew Lin. Rouge: A package for automatic evalua-\n",
      "In Annual Meeting of the Association\n",
      "\n",
      "tion of summaries.\n",
      "for Computational Linguistics, pages 74–81, 2004. 5\n",
      "[18] Kevin Lin, Linjie Li, Chung-Ching Lin, Faisal Ahmed, Zhe\n",
      "Gan, Zicheng Liu, Yumao Lu, and Lijuan Wang. Swinbert:\n",
      "End-to-end transformers with sparse attention for video cap-\n",
      "tioning. In IEEE/CVF Conference on Computer Vision and\n",
      "Pattern Recognition, pages 17949–17958, 2022. 2, 3, 6, 7\n",
      "\n",
      "[19] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang,\n",
      "Stephen Lin, and Han Hu. Video swin transformer.\n",
      "In\n",
      "IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition, pages 3192–3201, 2022. 2, 6\n",
      "\n",
      "[20] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan\n",
      "Duan, Tianrui Li, Jason Li, Taroon Bharti, and Ming Zhou.\n",
      "Univl: A unified video and language pre-training model for\n",
      "multimodal understanding and generation. arXiv preprint\n",
      "arXiv:2002.06353, 2020. 3, 6\n",
      "\n",
      "[21] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,\n",
      "Makarand Tapaswi,\n",
      "and Josef Sivic.\n",
      "Ivan Laptev,\n",
      "HowTo100M: Learning a text-video embedding by watching\n",
      "hundred million narrated video clips. In IEEE/CVF Interna-\n",
      "tional Conference on Computer Vision, pages 2630–2640,\n",
      "2019. 3\n",
      "\n",
      "[22] Boxiao Pan, Haoye Cai, De-An Huang, Kuan-Hui Lee,\n",
      "Adrien Gaidon, Ehsan Adeli, and Juan Carlos Niebles.\n",
      "Spatio-temporal graph for video captioning with knowledge\n",
      "distillation. In IEEE/CVF Conference on Computer Vision\n",
      "and Pattern Recognition, pages 10867–10876, 2020. 1, 6\n",
      "[23] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing\n",
      "Zhu. Bleu: a method for automatic evaluation of machine\n",
      "translation. In Annual Meeting of the Association for Com-\n",
      "putational Linguistics, pages 311–318, 2002. 5\n",
      "\n",
      "[24] Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian\n",
      "Metze, Alexander Hauptmann, Joao Henriques, and Andrea\n",
      "Vedaldi. Support-set bottlenecks for video-text representa-\n",
      "tion learning. arXiv preprint arXiv:2010.02824, 2020. 6\n",
      "[25] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\n",
      "Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\n",
      "Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen\n",
      "Krueger, and Ilya Sutskever. Learning transferable visual\n",
      "models from natural language supervision. In International\n",
      "Conference on Machine Learning, pages 8748–8763, 2021.\n",
      "6\n",
      "\n",
      "[26] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\n",
      "Faster r-cnn: Towards real-time object detection with region\n",
      "\n",
      "15566\f[39] Qi Zheng, Chaoyue Wang, and Dacheng Tao. Syntax-aware\n",
      "In IEEE/CVF Con-\n",
      "action targeting for video captioning.\n",
      "ference on Computer Vision and Pattern Recognition, pages\n",
      "13093–13102, 2020. 6\n",
      "\n",
      "proposal networks. Advances in Neural Information Pro-\n",
      "cessing Systems, 28, 2015. 7\n",
      "\n",
      "[27] Hobin Ryu, Sunghun Kang, Haeyong Kang, and Chang D.\n",
      "Yoo. Semantic grouping network for video captioning. In\n",
      "Association for the Advancement of Artificial Intelligence,\n",
      "pages 2514–2522, 2021. 2, 3, 6, 7\n",
      "\n",
      "[28] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\n",
      "Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\n",
      "Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\n",
      "Open dataset of clip-filtered 400 million image-text pairs.\n",
      "arXiv preprint arXiv:2111.02114, 2021. 6\n",
      "\n",
      "[29] Paul Hongsuck Seo, Arsha Nagrani, Anurag Arnab, and\n",
      "Cordelia Schmid. End-to-end generative pretraining for mul-\n",
      "timodal video captioning. In IEEE/CVF Conference on Com-\n",
      "puter Vision and Pattern Recognition, pages 17959–17968,\n",
      "2022. 2, 3, 6\n",
      "\n",
      "[30] Alok Singh, Thoudam Doren Singh, and Sivaji Bandyopad-\n",
      "hyay. NITS-VC system for vatex video captioning challenge\n",
      "2020. arXiv preprint arXiv:2006.04058, 2020. 6\n",
      "\n",
      "[31] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi\n",
      "Parikh. Cider: Consensus-based image description evalu-\n",
      "In IEEE/CVF Conference on Computer Vision and\n",
      "ation.\n",
      "Pattern Recognition, pages 4566–4575, 2015. 5, 6\n",
      "\n",
      "[32] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang\n",
      "Wang, and William Yang Wang. Vatex: A large-scale, high-\n",
      "quality multilingual dataset for video-and-language research.\n",
      "In IEEE/CVF International Conference on Computer Vision,\n",
      "2019. 5, 6\n",
      "\n",
      "[33] Chao-Yuan Wu, Manzil Zaheer, Hexiang Hu, R Manmatha,\n",
      "Alexander J Smola, and Philipp Kr¨ahenb¨uhl. Compressed\n",
      "video action recognition. In IEEE/CVF Conference on Com-\n",
      "puter Vision and Pattern Recognition, pages 6026–6035,\n",
      "2018. 2\n",
      "\n",
      "[34] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. MSR-VTT: A\n",
      "large video description dataset for bridging video and lan-\n",
      "In IEEE/CVF Conference on Computer Vision and\n",
      "guage.\n",
      "Pattern Recognition, pages 5288–5296, 2016. 5\n",
      "\n",
      "[35] Shen Yan, Tao Zhu, Zirui Wang, Yuan Cao, Mi Zhang, So-\n",
      "ham Ghosh, Yonghui Wu, and Jiahui Yu. Video-text model-\n",
      "ing with zero-shot transfer from contrastive captioners. arXiv\n",
      "preprint arXiv:2212.04979, 2022. 6\n",
      "\n",
      "[36] Hanhua Ye, Guorong Li, Yuankai Qi, Shuhui Wang, Qing-\n",
      "ming Huang, and Ming-Hsuan Yang. Hierarchical modu-\n",
      "lar network for video captioning. In IEEE/CVF Conference\n",
      "on Computer Vision and Pattern Recognition, pages 17939–\n",
      "17948, 2022. 1, 2, 3, 6, 7\n",
      "\n",
      "[37] Ziqi Zhang, Zhongang Qi, Chunfeng Yuan, Ying Shan, Bing\n",
      "Li, Ying Deng, and Weiming Hu. Open-book video caption-\n",
      "ing with retrieve-copy-generate network. In IEEE/CVF Con-\n",
      "ference on Computer Vision and Pattern Recognition, pages\n",
      "9837–9846, 2021. 6\n",
      "\n",
      "[38] Ziqi Zhang, Yaya Shi, Chunfeng Yuan, Bing Li, Peijin Wang,\n",
      "Weiming Hu, and Zheng-Jun Zha. Object relational graph\n",
      "with teacher-recommended learning for video captioning.\n",
      "In IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition, pages 13275–13285, 2020. 1, 3, 6\n",
      "\n",
      "15567\f\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "# Extracting text\n",
    "extracted_text = extract_text_from_pdf(pdf_url)\n",
    "# save extracted text to pdf_path[:-4] + '.txt'\n",
    "with open(pdf_url[:-4] + '.txt', 'w') as f:\n",
    "    f.write(extracted_text)\n",
    "\n",
    "# Print extracted text\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "process-papers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
